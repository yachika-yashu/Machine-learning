{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYzcrVwlx/6tDoCrTjC0Lr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yachika-yashu/Machine-learning/blob/main/stochastic_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ESMAjJyOs5c0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train,y_train)\n",
        "print(reg.coef_)\n",
        "print(reg.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB8AxYMsB8k9",
        "outputId": "36d1985f-6348-45c9-f026-82cf5fee2164"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(442, 10)\n",
            "(442,)\n",
            "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
            "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238]\n",
            "151.88331005254167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WYHjQsWCAT4",
        "outputId": "90323639-13b4-4c42-fb68-13506161f4fd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(353, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWrQ6QP8CBkb",
        "outputId": "9bd276f2-0c8b-486a-8f35-938618bb6dcd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4399338661568968"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stochastic Gradient descent**"
      ],
      "metadata": {
        "id": "ZngEO6brCHtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference between the SGDRegressor and GDRegressor lies in how they update model parameters during training. The SGDRegressor uses Stochastic Gradient Descent, where it updates the weights and intercept for each training epoch by selecting a single random sample from the dataset. Within each epoch, it runs an inner loop where it randomly picks a data point (idx) and computes the prediction y_hat using just that sample. It then calculates gradients (for both intercept and coefficients) based on the error from that sample and updates the model parameters immediately. This leads to more frequent updates (once per sample) and introduces some noise but allows faster convergence on large datasets.\n",
        "\n",
        "On the other hand, GDRegressor uses Batch Gradient Descent, which updates the parameters only once per epoch using the entire training dataset. It calculates predictions for all samples at once (y_hat = np.dot(X_train, self.coef_) + self.intercept_) and then computes the average gradient across the full dataset. These gradients are then used to update the coefficients and intercept. This makes the learning process smoother and more stable but can be computationally heavier and slower for very large datasets since it processes all samples in each epoch. Additionally, the fit() method in SGDRegressor involves more nested loops and random sampling logic, while GDRegressor uses a simpler, cleaner structure due to its batch-based approach.\n"
      ],
      "metadata": {
        "id": "8D5GW6qbDDG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDRegressor:\n",
        "\n",
        "    def __init__(self,learning_rate=0.01,epochs=100):\n",
        "\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self,X_train,y_train):\n",
        "        # init your coefs\n",
        "        self.intercept_ = 0\n",
        "        self.coef_ = np.ones(X_train.shape[1])\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            for j in range(X_train.shape[0]):\n",
        "                idx = np.random.randint(0,X_train.shape[0]) #selecting random row\n",
        "\n",
        "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_  #prediction for that selected row\n",
        "\n",
        "                intercept_der = -2 * (y_train[idx] - y_hat)\n",
        "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
        "\n",
        "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
        "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
        "\n",
        "        print(self.intercept_,self.coef_)\n",
        "\n",
        "    def predict(self,X_test):\n",
        "        return np.dot(X_test,self.coef_) + self.intercept_"
      ],
      "metadata": {
        "id": "PJqbNjpXCLmB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdr = SGDRegressor(epochs=1000,learning_rate=0.5)\n",
        "gdr.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqdoz4n5CRTr",
        "outputId": "a29cd845-8a36-4727-b77b-c544e9b5898f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151.5531043227468 [  25.52560977  -82.08266568  524.53664661  311.27344914 -996.63042266\n",
            "  647.18806302   91.14400109  131.88414053  892.85249008  148.28071358]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = gdr.predict(X_test)"
      ],
      "metadata": {
        "id": "lcY14j3gCSmC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxhQnLV-CTwp",
        "outputId": "d55c38af-870c-4c61-f6ab-4ee0c6957008"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4127502223048788"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}